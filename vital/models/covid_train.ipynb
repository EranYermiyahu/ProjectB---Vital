{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training covid models\n",
    "### This notebook is an example usage of how to use the model alongside the covid-data-collector in order to train, evaluate and test the model\n",
    "#### In this notebook you will find example usages on how to use the core functionalities of the model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import third party modules, and also the data_collector: covid19_genome and the model module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 17:38:13.846578: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 17:38:13.909467: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-04 17:38:14.277567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2024-03-04 17:38:14.277624: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2024-03-04 17:38:14.277631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Uncomment to disable GPU\n",
    "import glob\n",
    "\n",
    "from model import Model, DatasetName, load_model, remove_model\n",
    "\n",
    "__ORIG_WD__ = os.getcwd()\n",
    "\n",
    "os.chdir(f\"{__ORIG_WD__}/../data_collectors/\")\n",
    "from covid19_genome import Covid19Genome\n",
    "\n",
    "os.chdir(__ORIG_WD__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model, or try to load it, if it was already have been created.\n",
    "\n",
    "In order to use the model, the first thing you have to do is provide it with a dataset (with the help of the data_collector). In the following cell you are provided with an example that create the dataset.\n",
    "\n",
    "You should note that when you are creating the dataset, you are passing the dataset type. You can obtain the available dataset types in the system by calling the model class function ```get_ds_types()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "++++\n",
      "./data/cov19-1024e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 17:38:15.369719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.384292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.384416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.384804: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 17:38:15.385078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.385165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.385235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.419877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.419996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.420072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-03-04 17:38:15.420140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22461 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cov19-1024e\"\n",
    "\n",
    "try:\n",
    "    print(\"loading model\")\n",
    "    model = load_model(model_name)\n",
    "except Exception:\n",
    "    print(\"creating model\")\n",
    "    covid19_genome = Covid19Genome()\n",
    "    lineages = covid19_genome.getLocalLineages(1024)\n",
    "    lineages.sort()\n",
    "    dataset = []\n",
    "    def get_dataset():\n",
    "        for lineage in lineages:\n",
    "            dataset.append((lineage, covid19_genome.getLocalAccessionsPath(lineage)))\n",
    "        return dataset\n",
    "\n",
    "    portions = {\n",
    "        DatasetName.trainset.name: 0.8,\n",
    "        DatasetName.validset.name: 0.1,\n",
    "        DatasetName.testset.name: 0.1\n",
    "    }\n",
    "\n",
    "    dataset = get_dataset()\n",
    "    model = Model(model_name)\n",
    "    model.create_datasets(model.get_ds_types()[0], dataset, portions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created the model, and created its datasets. You can check which neural network structures is available. You can do that by calling the model class function ```get_ml_model_structure()```.\n",
    "\n",
    "After you see all the ml_model structures available in the system, you can check which hyper parameters are needed to define each and every ml_model structure. This is done by calling the model class function ```get_ml_model_structure_hps()```. The ```get_ml_model_structure_hps()``` will return which hps are required, and what it their type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VitStructure', 'ConvStructure', 'VitStructure_ex', 'CLSTMStructure']\n",
      "{'convclass': 'required', 'convnet': 'required', 'labels': 'required', 'seq_len': 'required', 'regularizer': 'optional', 'initializer': 'optional', 'd_model': 'required', 'activation': 'optional'}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_ml_model_structures())\n",
    "print(model.get_ml_model_structure_hps(model.get_ml_model_structures()[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see which properties help define the current type of dataset by calling to the model class function ```get_ds_props()``` This function could be called only after the dataset have been succesfully created. This function will return the properties of the dataset as well as their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coverage': 4, 'substitution_rate': 0.005, 'insertion_rate': 0.001, 'deletion_rate': 0.001, 'read_length': 128, 'frag_len': 128, 'num_frags': 256}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_ds_props())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A use case of the system with the VitStructure model and the minhash genome datasets (a.k.a. mh_genome_ds).\n",
    "\n",
    "In the mh_genome_ds the coverage is a dataset property that sets the genome coverage rate.\n",
    "\n",
    "In the VitStructure, the model_depth is the number of transformer encoders.\n",
    "\n",
    "In this example use-case these two parameters will help us define a neural network that will be trained on the dataset (with the current coverage rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = 4\n",
    "ml_model_depth = 1\n",
    "sequencer_instrument = \"illumina\"\n",
    "batch_size = 1024\n",
    "mini_batch_size = 256\n",
    "\n",
    "def write_to_res_file(results_file, model, ml_model_name):\n",
    "    categorical_acc = float(model.ml_models[ml_model_name].net.metrics[0].result())\n",
    "    loss = float(model.ml_models[ml_model_name].net.metrics[1].result())\n",
    "    results_file.write(f\"\\nCategorical Accuracy is {categorical_acc}\\n\")\n",
    "    results_file.write(f\"\\nloss is {loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vitex.1.4xxxx.illumina\n"
     ]
    }
   ],
   "source": [
    "sequencer_instrument_to_error_profile_map = {\n",
    "    \"illumina\": {\n",
    "        \"substitution_rate\": 0.005,\n",
    "        \"insertion_rate\": 0.001,\n",
    "        \"deletion_rate\": 0.001\n",
    "    },\n",
    "    \"ont\": {\n",
    "        \"substitution_rate\": 0.01,\n",
    "        \"insertion_rate\": 0.04,\n",
    "        \"deletion_rate\": 0.04\n",
    "    },\n",
    "    \"pacbio\": {\n",
    "        \"substitution_rate\": 0.005,\n",
    "        \"insertion_rate\": 0.025,\n",
    "        \"deletion_rate\": 0.025\n",
    "    },\n",
    "    \"roche\": {\n",
    "        \"substitution_rate\": 0.002,\n",
    "        \"insertion_rate\": 0.01,\n",
    "        \"deletion_rate\": 0.01\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_model_name(ml_model_depth, coverage, sequencer_instrument):\n",
    "    if not sequencer_instrument in sequencer_instrument_to_error_profile_map:\n",
    "        raise Exception(f\"Invalid sequencer instrument: {sequencer_instrument}\")\n",
    "    return f\"vitex.{ml_model_depth}.{coverage}xxxx.{sequencer_instrument}\"\n",
    "\n",
    "ml_model_name = get_model_name(ml_model_depth, coverage, sequencer_instrument)\n",
    "print(ml_model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a new neural network\n",
    "\n",
    "In this cell we will create an ml_model with the required hps (and also optional) as outputted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled the model with the following metrics: ['categorical_accuracy', 'AUC']\n",
      "Heyyyyy\n",
      "input size: (1, 256, 128, 4)\n",
      "x after embedding size: (1, 256, 128, 1)\n",
      "x before encoder size: (1, 256, 128)\n",
      "x after encoder size: (1, 256, 128)\n",
      "x after CNNEX size: (1, 115)\n",
      "output size: (1, 188)\n",
      "Heyyyyy\n",
      "input size: (1, 256, 128, 4)\n",
      "x after embedding size: (1, 256, 128, 1)\n",
      "x before encoder size: (1, 256, 128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after encoder size: (1, 256, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 17:38:16.706540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-04 17:38:16.754462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after CNNEX size: (1, 115)\n",
      "output size: (1, 188)\n"
     ]
    }
   ],
   "source": [
    "newly_added = True\n",
    "try:\n",
    "    model.add_ml_model(ml_model_name, hps={\n",
    "        \"structure\": model.get_ml_model_structures()[2],\n",
    "        \"d_model\": model.get_ds_props()[\"frag_len\"],\n",
    "        \"seq_len\": model.get_ds_props()[\"num_frags\"],\n",
    "        \"d_val\": 128,\n",
    "        \"d_key\": 128,\n",
    "        \"heads\": 8,\n",
    "        \"d_ff\": 128+256,\n",
    "        \"convclass\": \"resnet_v2\",\n",
    "        \"convnet\": \"ResNet152V2\",\n",
    "        \"labels\":  len(model.get_labels()),\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"params\": {\n",
    "                \"learning_rate\": 0.001,\n",
    "            },\n",
    "        },\n",
    "        \"encoder_repeats\": ml_model_depth,\n",
    "        # \"LSTM_repeats\":  ml_model_depth,\n",
    "        # \"LSTM_units\": 64,\n",
    "        \"pooling_width\": 2,\n",
    "        \"kernel_width\": 3,\n",
    "        \"regularizer\": {\n",
    "            \"name\": \"l2\",\n",
    "            \"params\": {\n",
    "                \"l2\": 0.0001\n",
    "            }\n",
    "        },\n",
    "        \"dropout\": 0.2,\n",
    "        \"initializer\": \"he_normal\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "    newly_added = False\n",
    "    print(\"Model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_model_class = \"efficientnet\"\n",
    "# ml_model_name = \"efficient_net_b0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VitStructure', 'ConvStructure', 'VitStructure_ex', 'CLSTMStructure']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_ml_model_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# try:\n",
    "#     model.add_ml_model(ml_model_name, hps={\n",
    "#         \"structure\": model.get_ml_model_structures()[-1],\n",
    "#         \"convclass\": ml_model_class,\n",
    "#         \"convnet\": ml_model_name,\n",
    "#         \"seq_len\": model.get_ds_props()[\"num_frags\"],\n",
    "#         \"d_model\": model.get_ds_props()[\"frag_len\"],\n",
    "#         \"labels\":  len(model.get_labels()),\n",
    "#         \"batch_size\": batch_size,\n",
    "#         \"mini_batch_size\": mini_batch_size,\n",
    "#     })\n",
    "# except:\n",
    "#     # print exception info\n",
    "#     print(sys.exc_info()[0])\n",
    "#     print(sys.exc_info()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vit.1.1xxxx.illumina', 'clstm.1.4xxxx.illumina', 'conv.1.4xxxx.illumina', 'vitex.1.4xxxx.illumina']\n"
     ]
    }
   ],
   "source": [
    "models = model.list_ml_models()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if newly_added:\n",
    "#     assert False, \"Please consider doing transfer learning\"\n",
    "# # model.transfer(get_model_name(ml_model_depth, coverage * 2, sequencer_instrument), ml_model_name, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.change_ml_hps(ml_model_name, {\n",
    "#     \"regularizer\": {\n",
    "#         \"name\": \"l2\",\n",
    "#         \"params\": {\n",
    "#             \"l2\": 0.00005,\n",
    "#         },\n",
    "#     },\n",
    "#     \"optimizer\": {\n",
    "#         \"name\": \"AdamW\",\n",
    "#         \"params\": {\n",
    "#             \"learning_rate\": 0.0002,\n",
    "#         },\n",
    "#     },\n",
    "# })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updating the dataset coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_ds_props({\n",
    "    \"coverage\": coverage,\n",
    "    } | sequencer_instrument_to_error_profile_map[sequencer_instrument])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting dataset batch size and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_batch_size = batch_size\n",
    "# while(True):\n",
    "#     try:\n",
    "#         model.set_ds_batch_size(mini_batch_size)\n",
    "#         model.train(ml_model_name, epochs=30)\n",
    "#     except:\n",
    "#         mini_batch_size = int(mini_batch_size // 2)\n",
    "#         model.change_ml_hps(ml_model_name, {\n",
    "#             \"batch_size\": mini_batch_size,\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "Heyyyyy\n",
      "input size: (None, 256, 128, 4)\n",
      "x after embedding size: (None, 256, 128, 1)\n",
      "x before encoder size: (None, 256, 128)\n",
      "x after encoder size: (None, 256, 128)\n",
      "x after CNNEX size: (None, 115)\n",
      "output size: (None, 188)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 17:38:22.058800: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape invit_structure_ex/sequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-03-04 17:38:29.897635: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f5241808e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-04 17:38:29.897655: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-03-04 17:38:29.900196: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-04 17:38:29.963161: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mset_ds_batch_size(mini_batch_size)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# results = open('results.txt','a')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# results.write(f\"Model Name is : {ml_model_name}\\n\\n\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# results.write(f\"Model Depth is : {ml_model_depth}\\n\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# results.write(f\"Coverage is : {coverage}\\n\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# results.write(f\"Train Results : {ml_model_name}\\n\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# write_to_res_file(results, model, model_name)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# results.write(f\"Validation Results : {ml_model_name}\\n\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(ml_model_name)\n",
      "File \u001b[0;32m/host/home/user_7321/user_7321_backup/user_7321/docker/ProjectB---Vital/vital/models/model.py:326\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, ml_model_name, epochs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ml_model_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mml_models:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_ml_model(ml_model_name)\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mml_model_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDatasetName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mDatasetName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/host/home/user_7321/user_7321_backup/user_7321/docker/ProjectB---Vital/vital/models/ml_models/ml_model.py:197\u001b[0m, in \u001b[0;36mMLModel.train\u001b[0;34m(self, epochs, trainset, shuffle_buffer_size, validset)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    190\u001b[0m         trainset\u001b[38;5;241m.\u001b[39mget_tf_dataset(shuffle_buffer_size\u001b[38;5;241m=\u001b[39mshuffle_buffer_size),\n\u001b[1;32m    191\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhps\u001b[38;5;241m.\u001b[39mget(HPs\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[MLModelSaveCallback(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshuffle_buffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMLModelSaveCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:952\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    947\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    948\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    949\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    950\u001b[0m           args, kwds))\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m--> 952\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    953\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds, inner_filtered_flat_args):\n\u001b[1;32m    957\u001b[0m   \u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.set_ds_batch_size(mini_batch_size)\n",
    "# results = open('results.txt','a')\n",
    "# results.write(f\"Model Name is : {ml_model_name}\\n\\n\")\n",
    "# results.write(f\"Model Depth is : {ml_model_depth}\\n\")\n",
    "# results.write(f\"Coverage is : {coverage}\\n\")\n",
    "\n",
    "model.train(ml_model_name, epochs=30)\n",
    "# results.write(f\"Train Results : {ml_model_name}\\n\")\n",
    "# write_to_res_file(results, model, model_name)\n",
    "# results.write(f\"Validation Results : {ml_model_name}\\n\")\n",
    "model.evaluate(ml_model_name)\n",
    "# write_to_res_file(results, model, model_name)\n",
    "# results.write(f\"Test Results : {ml_model_name}\\n\")\n",
    "model.test(ml_model_name)\n",
    "# write_to_res_file(results, model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00754310330376029\n",
      "4.746018886566162\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model.evaluate(ml_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# model.test(ml_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
