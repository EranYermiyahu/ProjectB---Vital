{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training covid models\n",
    "### This notebook is an example usage of how to use the model alongside the covid-data-collector in order to train, evaluate and test the model\n",
    "#### In this notebook you will find example usages on how to use the core functionalities of the model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import third party modules, and also the data_collector: covid19_genome and the model module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:21:07.317608: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-25 21:21:07.427599: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-25 21:21:07.797043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-25 21:21:07.797085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-25 21:21:07.871357: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-25 21:21:08.027702: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-25 21:21:08.029875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-25 21:21:08.847536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/host/home/user_7321/user_7321_backup/user_7321/docker/ProjectB---Vital/vital/models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Uncomment to disable GPU\n",
    "import glob\n",
    "\n",
    "from model import Model, DatasetName, load_model, remove_model\n",
    "\n",
    "__ORIG_WD__ = os.getcwd()\n",
    "print(__ORIG_WD__)\n",
    "\n",
    "os.chdir(f\"{__ORIG_WD__}/../data_collectors/\")\n",
    "from covid19_genome import Covid19Genome\n",
    "\n",
    "os.chdir(__ORIG_WD__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a model, or try to load it, if it was already have been created.\n",
    "\n",
    "In order to use the model, the first thing you have to do is provide it with a dataset (with the help of the data_collector). In the following cell you are provided with an example that create the dataset.\n",
    "\n",
    "You should note that when you are creating the dataset, you are passing the dataset type. You can obtain the available dataset types in the system by calling the model class function ```get_ds_types()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "++++\n",
      "./data/cov19-1024e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:21:22.197689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-25 21:21:22.372639: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cov19-1024e\"\n",
    "\n",
    "try:\n",
    "    print(\"loading model\")\n",
    "    model = load_model(model_name)\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "    print(\"creating model\")\n",
    "    covid19_genome = Covid19Genome()\n",
    "    lineages = covid19_genome.getLocalLineages(1024)\n",
    "    lineages.sort()\n",
    "    dataset = []\n",
    "    def get_dataset():\n",
    "        for lineage in lineages:\n",
    "            dataset.append((lineage, covid19_genome.getLocalAccessionsPath(lineage)))\n",
    "        return dataset\n",
    "\n",
    "    portions = {\n",
    "        DatasetName.trainset.name: 0.8,\n",
    "        DatasetName.validset.name: 0.1,\n",
    "        DatasetName.testset.name: 0.1\n",
    "    }\n",
    "\n",
    "    dataset = get_dataset()\n",
    "    model = Model(model_name)\n",
    "    model.create_datasets(model.get_ds_types()[0], dataset, portions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have created the model, and created its datasets. You can check which neural network structures is available. You can do that by calling the model class function ```get_ml_model_structure()```.\n",
    "\n",
    "After you see all the ml_model structures available in the system, you can check which hyper parameters are needed to define each and every ml_model structure. This is done by calling the model class function ```get_ml_model_structure_hps()```. The ```get_ml_model_structure_hps()``` will return which hps are required, and what it their type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VitStructure', 'ConvStructure', 'VitStructure_ex', 'CLSTMStructure']\n",
      "{'d_model': 'required', 'd_val': 'required', 'd_key': 'required', 'd_ff': 'required', 'heads': 'required', 'dropout_rate': 'optional', 'regularizer': 'optional', 'initializer': 'optional', 'activation': 'optional', 'LSTM_repeats': 'required', 'labels': 'required', 'LSTM_units': 'required'}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_ml_model_structures())\n",
    "print(model.get_ml_model_structure_hps(model.get_ml_model_structures()[-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see which properties help define the current type of dataset by calling to the model class function ```get_ds_props()``` This function could be called only after the dataset have been succesfully created. This function will return the properties of the dataset as well as their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coverage': 4, 'substitution_rate': 0.005, 'insertion_rate': 0.001, 'deletion_rate': 0.001, 'read_length': 128, 'frag_len': 128, 'num_frags': 256}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_ds_props())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A use case of the system with the VitStructure model and the minhash genome datasets (a.k.a. mh_genome_ds).\n",
    "\n",
    "In the mh_genome_ds the coverage is a dataset property that sets the genome coverage rate.\n",
    "\n",
    "In the VitStructure, the model_depth is the number of transformer encoders.\n",
    "\n",
    "In this example use-case these two parameters will help us define a neural network that will be trained on the dataset (with the current coverage rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencer_instrument_to_error_profile_map = {\n",
    "    \"illumina\": {\n",
    "        \"substitution_rate\": 0.005,\n",
    "        \"insertion_rate\": 0.001,\n",
    "        \"deletion_rate\": 0.001\n",
    "    },\n",
    "    \"ont\": {\n",
    "        \"substitution_rate\": 0.01,\n",
    "        \"insertion_rate\": 0.04,\n",
    "        \"deletion_rate\": 0.04\n",
    "    },\n",
    "    \"pacbio\": {\n",
    "        \"substitution_rate\": 0.005,\n",
    "        \"insertion_rate\": 0.025,\n",
    "        \"deletion_rate\": 0.025\n",
    "    },\n",
    "    \"roche\": {\n",
    "        \"substitution_rate\": 0.002,\n",
    "        \"insertion_rate\": 0.01,\n",
    "        \"deletion_rate\": 0.01\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = 4\n",
    "ml_model_depth = 1\n",
    "# coverage_list = [1, 4]\n",
    "# ml_model_depth_list = [1, 2, 4]\n",
    "sequencer_instrument = \"illumina\"\n",
    "batch_size = 1024\n",
    "mini_batch_size = 256\n",
    "\n",
    "def get_model_name(ml_model_depth, coverage, sequencer_instrument):\n",
    "    if not sequencer_instrument in sequencer_instrument_to_error_profile_map:\n",
    "        raise Exception(f\"Invalid sequencer instrument: {sequencer_instrument}\")\n",
    "    return f\"clstm.{ml_model_depth}.{coverage}x.{sequencer_instrument}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_res_file(results_file, model, ml_model_name):\n",
    "    categorical_acc = float(model.ml_models[ml_model_name].net.metrics[0].result())\n",
    "    loss = float(model.ml_models[ml_model_name].net.metrics[1].result())\n",
    "    results_file.write(f\"\\nCategorical Accuracy is {categorical_acc}\\n\")\n",
    "    results_file.write(f\"\\nloss is {loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamaters exploration\n",
    "# Search Grid: Dropout, learning rate, regularizer\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coverage_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m coverage \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcoverage_list\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ml_model_depth \u001b[38;5;129;01min\u001b[39;00m ml_model_depth_list:\n\u001b[1;32m      4\u001b[0m         ml_model_name \u001b[38;5;241m=\u001b[39m get_model_name(ml_model_depth, coverage, sequencer_instrument)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coverage_list' is not defined"
     ]
    }
   ],
   "source": [
    "results = open('results.txt','a')\n",
    "for coverage in coverage_list:\n",
    "    for ml_model_depth in ml_model_depth_list:\n",
    "        ml_model_name = get_model_name(ml_model_depth, coverage, sequencer_instrument)\n",
    "        print(ml_model_name)\n",
    "        results.write(f\"Model Name is : {ml_model_name}\\n\\n\")\n",
    "        results.write(f\"Model Depth is : {ml_model_depth}\\n\")\n",
    "        results.write(f\"Coverage is : {coverage}\\n\")\n",
    "        newly_added = True\n",
    "        try:\n",
    "            model.add_ml_model(ml_model_name, hps={\n",
    "                \"structure\": model.get_ml_model_structures()[2],\n",
    "                \"d_model\": model.get_ds_props()[\"frag_len\"],\n",
    "                \"seq_len\": model.get_ds_props()[\"num_frags\"],\n",
    "                \"d_val\": 128,\n",
    "                \"d_key\": 128,\n",
    "                \"heads\": 8,\n",
    "                \"d_ff\": 128+256,\n",
    "                \"labels\":  len(model.get_labels()),\n",
    "                \"activation\": \"relu\",\n",
    "                \"optimizer\": {\n",
    "                    \"name\": \"Adam\",\n",
    "                    \"params\": {\n",
    "                        \"learning_rate\": 0.001,\n",
    "                    },\n",
    "                },\n",
    "                # \"encoder_repeats\": ml_model_depth,\n",
    "                \"LSTM_repeats\":  ml_model_depth,\n",
    "                \"LSTM_units\": 64,\n",
    "                \"regularizer\": {\n",
    "                    \"name\": \"l2\",\n",
    "                    \"params\": {\n",
    "                        \"l2\": 0.0001\n",
    "                    }\n",
    "                },\n",
    "                \"dropout\": 0.2,\n",
    "                \"initializer\": \"he_normal\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "            newly_added = False\n",
    "            print(\"Model already exists\")\n",
    "\n",
    "        model.update_ds_props({\"coverage\": coverage,} \n",
    "        | sequencer_instrument_to_error_profile_map[sequencer_instrument])\n",
    "        model.set_ds_batch_size(mini_batch_size)\n",
    "\n",
    "        model.train(ml_model_name, epochs=30)\n",
    "        results.write(f\"Train Results : {ml_model_name}\\n\")\n",
    "        write_to_res_file(results, model, model_name)\n",
    "        results.write(f\"Validation Results : {ml_model_name}\\n\")\n",
    "        model.evaluate(ml_model_name)\n",
    "        write_to_res_file(results, model, model_name)\n",
    "        results.write(f\"Test Results : {ml_model_name}\\n\")\n",
    "        model.test(ml_model_name)\n",
    "        write_to_res_file(results, model, model_name)\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "covit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
